<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium Web' rel='stylesheet'>
<link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/dreampulse/computer-modern-web-font/master/fonts.css">
<style type="text/css">
    body {
        font-family: "Computer Modern Sans", "Titillium Web", sans-serif;
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
    }

    h1 {
        font-family: "Computer Modern Sans", "Titillium Web", sans-serif;
        font-size:32px;
        font-weight:300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }
    img {
        max-width: 100%;
        max-height: auto;
    }
    /*img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }*/

    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
                15px 15px 0 0px #fff, /* The fourth layer */
                15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
                20px 20px 0 0px #fff, /* The fifth layer */
                20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
                25px 25px 0 0px #fff, /* The fifth layer */
                25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }

    .paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr
    {
        border: 0;
        height: 1px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
</style>

<html>
<head>
    <link rel="shortcut icon" type="image/png" href="./icon.png" />
    <title>Patch-VQ: 'Patching Up' the Video Quality Problem</title>
</head>

<body>
<br>
<div style="text-align: center;">
    <span style="font-size:38px">Patch-VQ: 'Patching Up' the Video Quality Problem<br></span>
    <br>
    <table align=center width=900px>
        <tr>
            <td align=center>
                  <span style="font-size:25px">
                      <a href="https://baidut.github.io">Zhenqiang Ying</a><sup>1*</sup>,
                      <a href="https://www.linkedin.com/in/maniratnam-mandal/">Maniratnam Mandal</a><sup>1*</sup>,
                      <a href="https://deeptigp.github.io/">Deepti Ghadiyaram</a><sup>2+</sup>,
                      <a href="https://www.ece.utexas.edu/people/faculty/alan-bovik">Alan Bovik</a><sup>1+</sup>
                  </span>
            </td>
        </tr>
        <tr>
            <td align="center">
						  <span style="font-size:22px">
                <!-- <br> -->
                <sup>*+</sup>Equal Contribution<br>
							  <sup>1</sup>UT Austin,<sup>2</sup>Facebook AI Research<br>
						  </span>
                <br>
                <span style="font-size:25px">
							  Accepted at <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>
						  </span>
            </td>
        </tr>
    </table>

    <table align=center width=700px>
        <tr>
<!--            <td align=center width=140px>-->
<!--                <div style="text-align: center;">-->
<!--                    <span style="font-size:22px"><a href='https://github.com/facebookresearch/sound-spaces' target="_blank"> [Code & Data]</a></span>-->
<!--                </div>-->
<!--            </td>-->
            <td align=center width=100px>
                <div style="text-align: center;">
                    <span style="font-size:22px"><a href='https://arxiv.org/abs/2011.13544' target="_blank"> [Paper]</a></span>
                    <span style="font-size:22px"><a href='https://baidut.github.io/PatchVQ/' target="_blank"> [Website]</a></span>
                </div>
            </td>
        </tr>
        <tr>
          <td>
            <div style="text-align: center; font-size:22px">
                <b>IFML Thrust Areas:</b> <i>Learning with Dynamic Data</i> and <i>Video</i>
            </div>
          </td>
        </tr>
    </table>
</div>
<br>
<br>
<table align=center width=900px>
    <tr>
        <td>
            <div style="text-align: center;">
                <img class="round" width="800px" src="https://github.com/mandal-cv/pvqpage/blob/main/fig1_proposal_v8.png?raw=true"/>
            </div>
            <br>
        </td>
    </tr>
    <tr>
        <td style="font-size:15pt; text-align:justify">
            No-reference (NR) perceptual video quality assessment (VQA) is a complex, unsolved, and important problem to social and streaming media applications. Efficient and accurate video quality predictors are needed to monitor and guide the processing of billions of shared, often imperfect, user-generated content (UGC). Unfortunately, current NR models are limited in their prediction capabilities on real-world, "in-the-wild" UGC video data. To advance progress on this problem, we created the largest (by far) subjective video quality dataset, containing 38,811 real-world distorted videos and 116,433 space-time localized video patches ("v-patches"), and 5.5M human perceptual quality annotations. Using this, we created two unique NR-VQA models: (a) a local-to-global region-based NR VQA architecture (called PVQ) that learns to predict global video quality and achieves state-of-the-art performance on 3 UGC datasets, and (b) a first-of-a-kind space-time video quality mapping engine (called PVQ Mapper) that helps localize and visualize perceptual distortions in space and time.
        </td>
    </tr>
</table>

<br>

<section>
    <header style="text-align: center;font-size: 30pt">LIVE-FB LSVQ Dataset</header>
    <hr>
    <table align=center width=900px>
        <tr>
            <td style="font-size:15pt; text-align:justify">
                <div>
                    The LIVE-FB Large-scale Social Video Quality (LSVQ) Dataset includes 38,811 videos and 116,433 “v-patches” extracted from them, on which we collected about 5.5M quality scores in total from around 6,300 unique subjects. The dataset was created by sampling from 400,000 "UGC-like" videos from Internet Archive and Yahoo-Flicker Creative Commons 100M datasets.
                    <br>
                    <br>
                </div>
            </td>
        </tr>
        <tr>
            <td>
                <div style="text-align: center; ">
                    <img class="round" width="700px" src="https://github.com/mandal-cv/pvqpage/blob/main/datset_examples2.png?raw=true"/>
                </div>
                <br>
                <div style="text-align: center; font-size:12pt">
                  A collage of video frames from the dataset, each resized to fit. The actual videos are of highly diverse sizes and resolutions.
                </div>
            </td>
        </tr>
    </table>
</section>
<br>

<section>
    <header style="text-align: center;font-size: 30pt">PVQ Mapper</header>
    <hr>
    <table align=center width=900px>
        <tr>
            <td style="font-size:15pt; text-align:justify">
                <div>
                    We created a state-of-the-art deep blind video quality predictor, using a deep neural architecture that computes 2D and 3D video features. The features feed a time series regressor that learns to accurately predict both global video quality, as well as local space-time v-patch quality, by exploiting the relations between them.
                    We also created another unique prediction model that predicts first-of-a-kind space-time maps of video quality. This second model, called the PVQ Mapper, helps localize, visualize, and act on video distortions.
                </div>
            </td>
        </tr>
        <tr>
            <td>
                <div style="text-align: center; font-size:10pt">
                    <img class="round" width="900px" src="https://github.com/mandal-cv/pvqpage/blob/main/patch-up13.png?raw=true"/>
                </div>
                <div style="text-align: center; font-size:12pt">
                    <b>A first of its kind video quality map predictor:</b> Space-time quality maps generated on a video using our PVQ Mapper, and sampled in time for display. Eight video frames are shown at top, with spatial quality maps (blended with the original frames using magma color) immediately under, while the bottom plots show the evolving quality of the video.
                </div>
            </td>
        </tr>

    </table>
</section>
<br>

<section>
    <header style="text-align: center;font-size: 30pt">Examples</header>
    <hr>
    <table align=center width=900px>
        <tr>
            <div style="text-align: center; font-size:15pt">
                Generated Space-time Quality Maps (Video frames on the top, generated maps below)
                <br>
                <br>
            </div>
        </tr>
        <tr>
            <td>
                <img class="round" width="400px" src="https://github.com/mandal-cv/pvqpage/blob/main/A021.gif?raw=true"/>
            </td>
            <td>
                <img class="round" width="400px" src="https://github.com/mandal-cv/pvqpage/blob/main/A037.gif?raw=true"/>
            </td>
        </tr>

        <tr>
            <td>
                <img class="round" width="400px" src="https://github.com/mandal-cv/pvqpage/blob/main/A021_map.gif?raw=true"/>
            </td>
            <td>
                <img class="round" width="400px" src="https://github.com/mandal-cv/pvqpage/blob/main/A037_map.gif?raw=true"/>
            </td>
        </tr>

    </table>
</section>
<br>

<section>
    <header style="text-align: center;font-size: 30pt">References</header>
    <hr>
    <table align=center width=900px>
        <tr>
            <td style="font-size:15pt; text-align:justify">
            <span>
                (1) Zhenqiang Ying, Maniratnam Mandal, Deepti Ghadiyaram, Alan Bovik
                <b>Patch-VQ: 'Patching Up' the Video Quality Problem</b>.
                In <i>CVPR</i> 2021 <a href="https://arxiv.org/abs/2011.13544" target="_blank" style="font-size:15pt;">[Paper]</a><a href="https://baidut.github.io/PatchVQ/" target="_blank" style="font-size:15pt;">[Website]</a>
            </span>
            </td>
        </tr>
        <tr>
            <td style="font-size:15pt; text-align:justify">
            <span>
                (2) Zhenqiang Ying, Haoran Niu, Praful Gupta, Dhruv Mahajan, Deepti Ghadiyaram, Alan Bovik.
                <b>From Patches to Pictures (PaQ-2-PiQ): Mapping the Perceptual Space of Picture Quality</b>.
                In <i>CVPR</i> 2020 <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Ying_From_Patches_to_Pictures_PaQ-2-PiQ_Mapping_the_Perceptual_Space_of_CVPR_2020_paper.pdf" target="_blank" style="font-size:15pt;">[Paper]</a><a href="https://baidut.github.io/PaQ-2-PiQ/" target="_blank" style="font-size:15pt;">[Website]</a>
            </span>
            </td>
        </tr>

        <tr>
            <td style="font-size:15pt; text-align:justify">
            <span>
                (3) Vlad Hosu, Franz Hahn, Mohsen Jenadeleh, Hanhe Lin, Hui Men, Tamás Szirányi, Shujun Li, Dietmar Saupe.
                <b>The Konstanz natural video database(KoNViD-1k)</b>.
                In Quality of Multimedia Experience <i>(QoMEX)</i> 2017 <a href="https://ieeexplore.ieee.org/document/7965673" target="_blank" style="font-size:15pt;">[Paper]</a><a href="http://database.mmsp-kn.de/konvid-1k-database.html" target="_blank" style="font-size:15pt;">[Website]</a>
            </span>
            </td>
        </tr>

        <tr>
            <td style="font-size:15pt; text-align:justify">
            <span>
                (4) Zeina Sinno and Alan C. Bovik.
                <b>Large-scale Study of Perceptual Video Quality</b>.
                IEEE Transactions on Image Processing, vol. 28, no. 2, pp. 612-627, Feb. 2019 <a href="https://ieeexplore.ieee.org/document/8463581" target="_blank" style="font-size:15pt;">[Paper]</a><a href="https://live.ece.utexas.edu/research/LIVEVQC/index.html" target="_blank" style="font-size:15pt;">[Website]</a>
            </span>
            </td>
        </tr>

    </table>
</section>

<hr>
<table style="font-size:14px" align=center>
  <tr>
      <td>
          <b>This research is supported by the NSF IFML grant and by Facebook AI Applications Research.</b>
      </td>
  </tr>
    <tr>
        <td>
            Copyright &copy; 2021 University of Texas at Austin
        </td>
    </tr>
</table>

</body>
</html>
